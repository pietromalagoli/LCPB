{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Group 2404 </h1></center>\n",
    "\n",
    "\n",
    "<center><style>\n",
    "table {\n",
    "    font-size: 24px;\n",
    "}\n",
    "</style></center>\n",
    "\n",
    "| Last Name          | First Name            |Student Number|\n",
    "|--------------------|-----------------------|----------------|\n",
    "| Malagoli           | Pietro                |2125711         |\n",
    "| Boccanera          | Eugenia               |2109310         |\n",
    "| Braidi             | Federico              |2122169         |\n",
    "| Lovato             | Matteo                |2104269         |           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# AdaBoost Algorithm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Gradient Boosting \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# XGBoost \n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance, to_graphviz, plot_tree\n",
    "print(\"XGBoost version:\",xgboost.__version__)\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "mycmap = \"winter\"\n",
    "mpl.rcParams['image.cmap'] = mycmap\n",
    "plt.rcParams['font.size'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "\n",
    "dname=\"./DATA/\"\n",
    "str0=\"_XGB_24.dat\"\n",
    "fnamex=dname+'x'+str0\n",
    "fnamey=dname+'y'+str0\n",
    "x = np.loadtxt(fnamex, delimiter=\" \",dtype=float)\n",
    "y = np.loadtxt(fnamey)\n",
    "y = y.astype(int)\n",
    "N,L = len(x), len(x[0])\n",
    "\n",
    "N_train = int(0.75*N)\n",
    "x_train,y_train = x[:N_train],y[:N_train]\n",
    "x_test,y_test = x[N_train:],y[N_train:]\n",
    "print(f\"N={N}, N_train={N_train}, L={L}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scat(ax,x,y,i=0,j=1,s=4,title=\"\"):\n",
    "    ax.scatter(x[:,i],x[:,j],s=s,c=y)\n",
    "    ax.set_xlabel(f\"feature {i}\")\n",
    "    ax.set_ylabel(f\"feature {j}\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "for n in range(5):\n",
    "    print(x[n],y[n])\n",
    "\n",
    "fig,AX = plt.subplots(2,2,figsize=(8.5,8.1))\n",
    "scat(AX[0,0],x_train,y_train,title=\"Train\")\n",
    "scat(AX[0,1],x_train,y_train,i=2,j=3,title=\"Train\")\n",
    "scat(AX[1,0],x_test,y_test,title=\"Test\")\n",
    "scat(AX[1,1],x_test,y_test,i=2,j=3,title=\"Test\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(clf=GradientBoostingClassifier(),show=False):\n",
    "    # GradientBoostingClassifier():\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   loss function = deviance(default) used in Logistic Regression\n",
    "    # XGBClassifier()\n",
    "    #   n_estimators = 100 (default)\n",
    "    #   max_depth = 3 (default?)\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_hat = clf.predict(x_test)\n",
    "    \n",
    "    #print(\"errors: {:.2f}%   Accuracy={:.3f}\".format(100*(1-clf.score(x_test, y_test)),clf.score(x_test, y_test)))\n",
    "    \n",
    "    if show: \n",
    "        S=50\n",
    "        dx = 1\n",
    "        x_seq=np.arange(-S,S+dx,dx)\n",
    "        nx = len(x_seq)\n",
    "        x_plot=np.zeros((nx*nx,L))\n",
    "        q=0\n",
    "        for i in range(nx):\n",
    "            for j in range(nx):\n",
    "                x_plot[q,:2] = [x_seq[i],x_seq[j]]\n",
    "                q+=1\n",
    "        y_plot= clf.predict(x_plot)\n",
    "\n",
    "        fig,AX = plt.subplots(1,2,figsize=(8.2,4))\n",
    "        scat(AX[0],x_plot[:],y_plot,s=dx,title=\"predicted\")\n",
    "        scat(AX[1],x_train[:],y_train,title=\"training set\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()     \n",
    "        dump_list = clf.get_booster().get_dump()\n",
    "        num_trees = len(dump_list)\n",
    "        print(\"num_trees=\",num_trees)\n",
    "        \n",
    "        fig, AX = plt.subplots(2,1,figsize=(12, 5))\n",
    "        for i in range(min(2,num_trees)):\n",
    "            ax=AX[i]\n",
    "            plot_tree(clf, num_trees=i, ax=ax)\n",
    "        fig.savefig(\"DATA/tree-classif.png\", dpi=400, pad_inches=0.02)   \n",
    "        plt.show()\n",
    "    \n",
    "    return clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nn(num_layers,num_neurons,activation_function):\n",
    "    input_shape = x.shape[1]\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(units=num_neurons, activation=activation_function, input_shape=(input_shape,)))\n",
    "    for i in range(num_layers-1):\n",
    "        model_nn.add(Dense(units=num_neurons, activation=activation_function))\n",
    "    model_nn.add(Dense(units=2,activation='softmax'))\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model_nn.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    return model_nn\n",
    "\n",
    "num_layers=[2]                      #num of nn layers(input and output excluded)\n",
    "num_neurons=[20]                        #num of neurons per layer\n",
    "activation_functions=['relu']         #activation function for the hidden layers\n",
    "max_acc=0\n",
    "max_desc=\"\"\n",
    "perc=0.7\n",
    "n_rep=30\n",
    "\n",
    "\n",
    "for nl in num_layers:\n",
    "    for nn in num_neurons:\n",
    "        for act in activation_functions:\n",
    "            accuracies_nn=[]\n",
    "            for i in range(n_rep):\n",
    "                num_indices=int(perc*len(x))        #get quantity of data points in training\n",
    "                data_indices = np.arange(len(x))    #get indices\n",
    "                np.random.shuffle(data_indices)     #shuffle indices\n",
    "                train_indices = data_indices[:num_indices]  #get training set indices\n",
    "                test_indices = data_indices[num_indices:]   #get test set indices\n",
    "                \n",
    "                x_train, y_train = x[train_indices], y[train_indices]\n",
    "                x_test, y_test = x[test_indices], y[test_indices]\n",
    "                y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "                y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "                print(len(x_train))\n",
    "                print(len(x_test))\n",
    "                \n",
    "                print(f\"--RUNNING ({i+1}/{n_rep})--n_lay: {nl}, n_neu: {nn}, act: {act}\")\n",
    "                model=build_nn(num_layers=nl,num_neurons=nn,activation_function=act)\n",
    "                model.fit(x_train, y_train, validation_data=(x_test, y_test), verbose=0)\n",
    "                accuracy_nn = model.evaluate(x_test, y_test, verbose=0)[1]  #accuracy is at index 1\n",
    "                accuracies_nn.append(accuracy_nn)\n",
    "                print(accuracy_nn)\n",
    "            mean_acc=np.mean(accuracies_nn)\n",
    "            if(mean_acc>max_acc):\n",
    "                max_acc = mean_acc\n",
    "                max_desc = f\"n_lay: {nl}, n_neu: {nn}, act: {act}\"\n",
    "            print(model.summary())\n",
    "print(f\"Best mean accuracy ({max_acc}) obtained by the model with parameters: {max_desc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_best_xg():\n",
    "    model_xg = XGBClassifier(seed=1,\n",
    "        objective='binary:logistic', \n",
    "        importance_type=\"gain\", #weight, cover, ...\n",
    "        reg_lambda=0.2,\n",
    "        gamma=0.1, \n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1)\n",
    "    return model_xg\n",
    "\n",
    "def build_best_nn():\n",
    "    input_shape = x.shape[1]\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(20, activation='relu', input_shape=(input_shape,)))\n",
    "    model_nn.add(Dense(20, activation='relu'))\n",
    "    model_nn.add(Dense(2, activation='softmax'))\n",
    "    opt = keras.optimizers.Adam()\n",
    "    model_nn.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "    return model_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = [0.03,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99]  #percentage of data to be used in training \n",
    "n_rep = 30                          #number of times training-test is repeated to get the statistic\n",
    "mean_acc_best_xg = []\n",
    "mean_acc_best_nn = []\n",
    "\n",
    "for perc in train_perc:\n",
    "    accuracies_best_xg = []\n",
    "    accuracies_best_nn = []\n",
    "    model_best_nn=build_best_nn()\n",
    "    model_best_xg=build_best_xg()\n",
    "    print(model_best_nn.summary())\n",
    "    for i in range(n_rep):\n",
    "        num_indices=int(perc*len(x))        #get quantity of data points in training\n",
    "        data_indices = np.arange(len(x))    #get indices\n",
    "        np.random.shuffle(data_indices)     #shuffle indices\n",
    "        train_indices = data_indices[:num_indices]  #get training set indices\n",
    "        test_indices = data_indices[num_indices:]   #get test set indices\n",
    "        #print(len(train_indices))\n",
    "        #print(len(test_indices))\n",
    "\n",
    "        x_train, y_train = x[train_indices], y[train_indices]\n",
    "        x_test, y_test = x[test_indices], y[test_indices]\n",
    "\n",
    "        #XGBoost\n",
    "        model_best_xg.fit(x_train, y_train)\n",
    "        accuracy_best_xg = model_best_xg.score(x_test, y_test)\n",
    "        accuracies_best_xg.append(accuracy_best_xg)\n",
    "\n",
    "        #Neural Network\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "        model_best_nn.fit(x_train, y_train, validation_data=(x_test, y_test), verbose=0)\n",
    "        prova=model_best_nn.evaluate(x_test, y_test, verbose=0)\n",
    "        print(prova)\n",
    "        accuracy_best_nn = prova[1]  #accuracy is at index 1\n",
    "        accuracies_best_nn.append(accuracy_best_nn)\n",
    "\n",
    "    mean_acc_best_xg.append(np.mean(accuracies_best_xg))\n",
    "    mean_acc_best_nn.append(np.mean(accuracies_best_nn))\n",
    "    print(f\"Average accuracy on test set with XGBoost, cross-validation and {perc} of the data as training: {np.mean(accuracies_best_xg)}\")\n",
    "    print(f\"Average accuracy on test set with dense NN and cross-validation and {perc} of the data as training: {np.mean(accuracies_best_nn)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_acc_best_xg)\n",
    "print(mean_acc_best_nn)\n",
    "\n",
    "plt.plot([ perc*100 for perc in train_perc],mean_acc_best_xg,c=\"r\",label=\"XGBoost\")\n",
    "plt.plot([ perc*100 for perc in train_perc],mean_acc_best_nn,c=\"g\",label=\"FFNeuralNetwork\")\n",
    "\n",
    "plt.xlabel(\"Percentage of dataset used for training\")\n",
    "plt.ylabel(\"Average validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
